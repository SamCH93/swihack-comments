---
title: 'Multilingual comments comparison - Analysis'
author: 'Samuel Pawel'
date: '21.02.2020'
output: html_document
---

<!-- setup -->
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      cache = FALSE,
                      warning = FALSE, 
                      message = FALSE, 
                      fig.height = 5, 
                      fig.align = "center")
```


### Libraries
```{r}
library(knitr)
library(tidyverse)
library(lubridate)
library(stringr)
library(jsonlite)
library(ggbeeswarm)
library(quanteda)
```


### Data
```{r}
articles <- read_json("../articles.json")
languages <- c("German", "Spanish", "French", "Italian", "Japanese",
               "Portuguese", "Chinese", "English", "Russian", "Arabic")

## aggregating article features
articles_summaries <- articles %>% 
  map(.f = function(article) {
    
    comments <- map(.x = languages, .f = function(lang) {
      nr_comments <- length(article$content[[lang]]$comments)
      article_exists <- !is.null(article$content[[lang]])
      data.frame(language = lang, nr_comments = nr_comments,
                 article_exists = article_exists)
    }) %>% 
      bind_rows()
    
    data.frame(title = article$TITLE,
               link = article$link,
               published = article$PUBLISHED,
               created = article$CREATED,
               subject = article$SUBJECT,
               authortag = article$AUTHORSTAG,
               author = article$AUTHORS,
               # we assume first mentioned category is most important category
               category = article$category[[1]][[1]]) %>% 
      cbind(., comments)
  }) %>% 
  bind_rows() %>% 
  as_tibble() %>% 
  mutate(published = parse_date_time(published, orders = "%d.%m.%Y %H:%M", tz = "CET"),
         created = parse_date_time(created, orders = "%d.%m.%Y %H:%M", tz = "CET"))
```

### Descriptive statistics
```{r}
## computing some summary statistics
articles_summaries %>% 
  group_by(language) %>% 
  summarise(`Number of articles` = sum(article_exists),
            `Total comments` = sum(nr_comments),
            `Avg number of comments per article` = round(`Total comments`/`Number of articles`, 1)) %>% 
  arrange(-`Avg number of comments per article`) %>% 
  kable()

```

### Plots
```{r fig.height = 7, fig.width = 10}
## distribution of comments per language and category
articles_summaries %>% 
  filter(article_exists == TRUE) %>% 
  mutate(category = fct_lump(f = category, n = 5)) %>%
  ggplot(aes(x = language, y = nr_comments, 
             color = language)) +
  geom_boxplot(alpha = 0.1) +
  geom_quasirandom(alpha = 0.7) + 
  guides(color = FALSE) + 
  facet_wrap(~ category, ncol = 3) +
  labs(x = "Language", y = "Number of comments") +
  coord_flip()

## time-series plot per category
articles_summaries %>% 
  mutate(category = fct_lump(f = category, n = 4)) %>%
  mutate(date = date(published)) %>% 
  group_by(date, category) %>%
  summarise(sum_comments = sum(nr_comments)) %>%
  ggplot(aes(x = date, y = sum_comments)) +
  geom_line() +
  facet_wrap(~ category) +
  labs(x = "Date", y = "Total number of comments")

## wordcloud plot
wordcloud_languages <- c("German", "English")
for (lang in wordcloud_languages) {
  print(lang)
  map(articles, function(article) {
    article$content[[lang]]$comments
  }) %>% 
  unlist() %>% 
  corpus() %>% 
  dfm(remove = c(stopwords(ifelse(lang == "English", "en", "de")), "dass"),
      remove_punct = TRUE, remove_numbers = TRUE) %>% 
  textplot_wordcloud(min_count = 6, random_order = FALSE,
                     rotation = 0.25, max_size = 8,
                     color = RColorBrewer::brewer.pal(4, "Dark2"))  
}
```

